{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from msalib import layers\n",
    "from msalib import network\n",
    "from msalib import train\n",
    "from msalib.utils import load_dataset\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(output, label):\n",
    "    \"\"\"Loss function (softmax cross entropy)\n",
    "\n",
    "    Arguments:\n",
    "        output {tf tensor} -- output from network\n",
    "        label {tf tensor} -- labels\n",
    "\n",
    "    Returns:\n",
    "        tf tensor -- loss\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.losses.softmax_cross_entropy(\n",
    "        logits=output, onehot_labels=label)\n",
    "\n",
    "\n",
    "def run_mnist(config):\n",
    "    \"\"\"Run mnist/fashion_mnist classification\n",
    "\n",
    "    Arguments:\n",
    "        config {dict} -- read from config.yml\n",
    "    \"\"\"\n",
    "\n",
    "    print('='*100)\n",
    "    print('Run CNN on MNIST')\n",
    "    print('='*100)\n",
    "    print('Config:')\n",
    "    for key, value in config.items():\n",
    "        print('{:20} ({})'.format(key, value))\n",
    "    print('='*100)\n",
    "\n",
    "    tf.logging.set_verbosity(\n",
    "        getattr(tf.logging, config['verbosity']))\n",
    "    tf.set_random_seed(config['seed'])\n",
    "\n",
    "    # Generate data\n",
    "    trainset, testset = load_dataset(\n",
    "        name=config['dataset_name'],\n",
    "        num_train=config['num_train'],\n",
    "        num_test=config['num_test'],\n",
    "        lift_dim=config['lift_dimension']\n",
    "    )\n",
    "\n",
    "    # Build network\n",
    "    input = tf.placeholder(\n",
    "        tf.float32, [None, 28, 28, config['lift_dimension']], name='input')\n",
    "    output = tf.placeholder(tf.float32, [None, 10], name='output')\n",
    "    net = network.Network(name='msa_net')\n",
    "\n",
    "    net.add(layers.Conv2D(\n",
    "        input_shape=input.shape[1:],\n",
    "        filters=config['num_channels'],\n",
    "        kernel_size=config['filter_size'],\n",
    "        padding=config['padding'],\n",
    "        activation=config['activation'],\n",
    "        msa_rho=config['rho'],\n",
    "        msa_reg=config['reg'],\n",
    "        name='conv2d_0')\n",
    "    )\n",
    "\n",
    "    net.add(layers.AveragePooling2D(\n",
    "        pool_size=2, strides=2, padding=config['padding'], name='avg_pool_0')\n",
    "    )\n",
    "\n",
    "    net.add(layers.Conv2D(\n",
    "        input_shape=input.shape[1:],\n",
    "        filters=config['num_channels'],\n",
    "        kernel_size=config['filter_size'],\n",
    "        padding=config['padding'],\n",
    "        activation=config['activation'],\n",
    "        msa_rho=config['rho'],\n",
    "        msa_reg=config['reg'],\n",
    "        name='conv2d_init')\n",
    "    )\n",
    "\n",
    "    net.add(layers.AveragePooling2D(\n",
    "        pool_size=2, strides=2, padding=config['padding'], name='avg_pool_1')\n",
    "    )\n",
    "\n",
    "    for n in range(config['num_layers']):\n",
    "        net.add(layers.ResidualConv2D(\n",
    "            filters=config['num_channels'],\n",
    "            kernel_size=config['filter_size'],\n",
    "            padding=config['padding'],\n",
    "            activation=config['activation'],\n",
    "            msa_rho=config['rho'],\n",
    "            msa_reg=config['reg'],\n",
    "            delta=config['delta'],\n",
    "            name='residualconv2d_{}'.format(n)))\n",
    "\n",
    "    net.add(layers.Lower(name='lower'))\n",
    "    net.add(layers.Flatten(name='flatten'))\n",
    "    net.add(layers.Dense(\n",
    "        units=10, msa_rho=config['rho'], msa_reg=config['reg'], name='dense'))\n",
    "\n",
    "    net.msa_compute_x(input)\n",
    "    net.msa_compute_p(output, loss_func)\n",
    "    net.summary()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # MSA trainer\n",
    "    msa_trainer = train.MSATrainer(\n",
    "        network=net,\n",
    "        name='MSA_trainer',\n",
    "        maxiter=config['msa_maxiter'],\n",
    "        perturb_init=config['msa_perturb_init'])\n",
    "    msa_trainer.initialize(sess)\n",
    "    msa_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # SGD trainer\n",
    "    sgd_trainer = train.BPTrainer(\n",
    "        network=net, name='SGD_trainer',\n",
    "        method='GradientDescentOptimizer',\n",
    "        args={'learning_rate': config['sgd_lr']})\n",
    "    sgd_trainer.initialize(sess)\n",
    "    sgd_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adagrad trainer\n",
    "    adagrad_trainer = train.BPTrainer(\n",
    "        network=net, name='Adagrad_trainer',\n",
    "        method='AdagradOptimizer',\n",
    "        args={'learning_rate': config['adagrad_lr']})\n",
    "    adagrad_trainer.initialize(sess)\n",
    "    adagrad_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])\n",
    "\n",
    "    # Adam trainer\n",
    "    Adam_trainer = train.BPTrainer(\n",
    "        network=net, name='Adam_trainer',\n",
    "        method='AdamOptimizer',\n",
    "        args={'learning_rate': config['adam_lr']})\n",
    "    Adam_trainer.initialize(sess)\n",
    "    Adam_trainer.train(\n",
    "        session=sess,\n",
    "        trainset=trainset,\n",
    "        testset=testset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_epochs=config['num_epochs'],\n",
    "        buffer_size=config['buffer_size'],\n",
    "        print_step=config['print_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "====================================================================================================\n",
      "Run CNN on MNIST\n",
      "====================================================================================================\n",
      "Config:\n",
      "dataset_name         (mnist)\n",
      "seed                 (1)\n",
      "verbosity            (WARN)\n",
      "print_step           (100)\n",
      "lift_dimension       (1)\n",
      "num_train            (60000)\n",
      "num_test             (10000)\n",
      "num_channels         (5)\n",
      "filter_size          (3)\n",
      "padding              (same)\n",
      "num_layers           (2)\n",
      "rho                  (0.03)\n",
      "reg                  (0.001)\n",
      "activation           (tanh)\n",
      "delta                (0.5)\n",
      "batch_size           (128)\n",
      "buffer_size          (512)\n",
      "num_epochs           (1)\n",
      "sgd_lr               (0.01)\n",
      "adagrad_lr           (0.01)\n",
      "adam_lr              (0.001)\n",
      "msa_maxiter          (10)\n",
      "msa_perturb_init     (0.0)\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_0 (Conv2D)            (None, 28, 28, 5)         50        \n",
      "_________________________________________________________________\n",
      "avg_pool_0 (AveragePooling2D (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_init (Conv2D)         (None, 14, 14, 5)         230       \n",
      "_________________________________________________________________\n",
      "avg_pool_1 (AveragePooling2D (None, 7, 7, 5)           0         \n",
      "_________________________________________________________________\n",
      "residualconv2d_0 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "residualconv2d_1 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "lower (Lower)                (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                500       \n",
      "=================================================================\n",
      "Total params: 1,240\n",
      "Trainable params: 1,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2019-05-27 13:42:15.942783: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-05-27 13:42:15.950522: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294605000 Hz\n",
      "2019-05-27 13:42:15.950807: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x27ec780 executing computations on platform Host. Devices:\n",
      "2019-05-27 13:42:15.950854: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "====================================================================================================\n",
      "Trainer: MSATrainer (MSA_trainer)\n",
      "Settings: {'maxiter': 10, 'perturb_init': 0.0}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3245041087522345\n",
      "Test loss: 2.3243458390235903\n",
      "Step     0 of   469: \n",
      "Train loss: 2.251601083803985\n",
      "Test loss: 2.251721012390266\n",
      "Step     5 of   469: \n",
      "Train loss: 1.538198397321216\n",
      "Test loss: 1.5384017582667076\n",
      "Step    10 of   469: \n",
      "Train loss: 1.1045078256372678\n",
      "Test loss: 1.105470321946225\n",
      "Step    15 of   469: \n",
      "Train loss: 0.9116388513880261\n",
      "Test loss: 0.9115235062979036\n",
      "Step    20 of   469: \n",
      "Train loss: 0.8169570423788943\n",
      "Test loss: 0.8171625869759058\n",
      "Step    25 of   469: \n",
      "Train loss: 0.7602604875120066\n",
      "Test loss: 0.7607012547678866\n",
      "Step    30 of   469: \n",
      "Train loss: 0.7229184611874112\n",
      "Test loss: 0.7237023216182903\n",
      "Step    35 of   469: \n",
      "Train loss: 0.6881335943432177\n",
      "Test loss: 0.6881960241471307\n",
      "Step    40 of   469: \n",
      "Train loss: 0.6703700312113358\n",
      "Test loss: 0.6701718917337515\n",
      "Step    45 of   469: \n",
      "Train loss: 0.6550580712698274\n",
      "Test loss: 0.655356424592309\n",
      "Step    50 of   469: \n",
      "Train loss: 0.6388610050839892\n",
      "Test loss: 0.6396449862395303\n",
      "Step    55 of   469: \n",
      "Train loss: 0.6270352091829655\n",
      "Test loss: 0.626168093944\n",
      "Step    60 of   469: \n",
      "Train loss: 0.6185165355771275\n",
      "Test loss: 0.6175281046810797\n",
      "Step    65 of   469: \n",
      "Train loss: 0.6108651140988883\n",
      "Test loss: 0.6108085874278667\n",
      "Step    70 of   469: \n",
      "Train loss: 0.6012003164170152\n",
      "Test loss: 0.5996275445667364\n",
      "Step    75 of   469: \n",
      "Train loss: 0.5895808399733851\n",
      "Test loss: 0.5888501579983759\n",
      "Step    80 of   469: \n",
      "Train loss: 0.5822208718223086\n",
      "Test loss: 0.5819709429801521\n",
      "Step    85 of   469: \n",
      "Train loss: 0.57622456777904\n",
      "Test loss: 0.5751379854598287\n",
      "Step    90 of   469: \n",
      "Train loss: 0.5683031243793035\n",
      "Test loss: 0.5689358696088953\n",
      "Step    95 of   469: \n",
      "Train loss: 0.5656674807354555\n",
      "Test loss: 0.566170269907531\n",
      "Step   100 of   469: \n",
      "Train loss: 0.5614590576644671\n",
      "Test loss: 0.5604840918617734\n",
      "Step   105 of   469: \n",
      "Train loss: 0.5524259277824628\n",
      "Test loss: 0.5533440001940323\n",
      "Step   110 of   469: \n",
      "Train loss: 0.549295807793989\n",
      "Test loss: 0.5498651670197309\n",
      "Step   115 of   469: \n",
      "Train loss: 0.5453842188847267\n",
      "Test loss: 0.5452207978499137\n",
      "Step   120 of   469: \n",
      "Train loss: 0.5416017548512604\n",
      "Test loss: 0.5415284158819813\n",
      "Step   125 of   469: \n",
      "Train loss: 0.5395527194112034\n",
      "Test loss: 0.5401004906428062\n",
      "Step   130 of   469: \n",
      "Train loss: 0.536581898139695\n",
      "Test loss: 0.5371723740787829\n",
      "Step   135 of   469: \n",
      "Train loss: 0.5358356399051214\n",
      "Test loss: 0.5367558280290183\n",
      "Step   140 of   469: \n",
      "Train loss: 0.53350989672087\n",
      "Test loss: 0.5341045811014661\n",
      "Step   145 of   469: \n",
      "Train loss: 0.5312683400461229\n",
      "Test loss: 0.5307194545107373\n",
      "Step   150 of   469: \n",
      "Train loss: 0.5293321927725259\n",
      "Test loss: 0.5296888924756292\n",
      "Step   155 of   469: \n",
      "Train loss: 0.5258042132955486\n",
      "Test loss: 0.5259975553569147\n",
      "Step   160 of   469: \n",
      "Train loss: 0.5205366586224508\n",
      "Test loss: 0.5212191078117339\n",
      "Step   165 of   469: \n",
      "Train loss: 0.5179164149498535\n",
      "Test loss: 0.5191225563570604\n",
      "Step   170 of   469: \n",
      "Train loss: 0.5170056228920564\n",
      "Test loss: 0.5177573299003859\n",
      "Step   175 of   469: \n",
      "Train loss: 0.5123911772744131\n",
      "Test loss: 0.5131419664722378\n",
      "Step   180 of   469: \n",
      "Train loss: 0.5122841960292751\n",
      "Test loss: 0.5117960627806388\n",
      "Step   185 of   469: \n",
      "Train loss: 0.50749823552067\n",
      "Test loss: 0.5078324243678884\n",
      "Step   190 of   469: \n",
      "Train loss: 0.5075498392521325\n",
      "Test loss: 0.5078902840614319\n",
      "Step   195 of   469: \n",
      "Train loss: 0.5041583672923556\n",
      "Test loss: 0.5046973377466202\n",
      "Step   200 of   469: \n",
      "Train loss: 0.5033667852817956\n",
      "Test loss: 0.5030495468842782\n",
      "Step   205 of   469: \n",
      "Train loss: 0.5029358262732878\n",
      "Test loss: 0.5018799355474569\n",
      "Step   210 of   469: \n",
      "Train loss: 0.499953619504379\n",
      "Test loss: 0.5008525562993551\n",
      "Step   215 of   469: \n",
      "Train loss: 0.49840809404850006\n",
      "Test loss: 0.49848284686015826\n",
      "Step   220 of   469: \n",
      "Train loss: 0.49659523974030706\n",
      "Test loss: 0.49688278669017855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   225 of   469: \n",
      "Train loss: 0.4957673425896693\n",
      "Test loss: 0.4958732517088874\n",
      "Step   230 of   469: \n",
      "Train loss: 0.49451214807518457\n",
      "Test loss: 0.49457962204844264\n",
      "Step   235 of   469: \n",
      "Train loss: 0.4921166934704376\n",
      "Test loss: 0.4927618523775521\n",
      "Step   240 of   469: \n",
      "Train loss: 0.4926631066758754\n",
      "Test loss: 0.4924122989177704\n",
      "Step   245 of   469: \n",
      "Train loss: 0.49086415515107623\n",
      "Test loss: 0.4904660663362277\n",
      "Step   250 of   469: \n",
      "Train loss: 0.4899554462251017\n",
      "Test loss: 0.4909431474693751\n",
      "Step   255 of   469: \n",
      "Train loss: 0.4888014793395996\n",
      "Test loss: 0.4894995843454943\n",
      "Step   260 of   469: \n",
      "Train loss: 0.4867824760533996\n",
      "Test loss: 0.48697429668095155\n",
      "Step   265 of   469: \n",
      "Train loss: 0.48523149753020983\n",
      "Test loss: 0.485996803489782\n",
      "Step   270 of   469: \n",
      "Train loss: 0.48493501392461485\n",
      "Test loss: 0.485340859677832\n",
      "Step   275 of   469: \n",
      "Train loss: 0.48269837690612016\n",
      "Test loss: 0.4830962687225665\n",
      "Step   280 of   469: \n",
      "Train loss: 0.48278380273762395\n",
      "Test loss: 0.4830165624113406\n",
      "Step   285 of   469: \n",
      "Train loss: 0.47917611720198294\n",
      "Test loss: 0.48061419972928904\n",
      "Step   290 of   469: \n",
      "Train loss: 0.4789215942560616\n",
      "Test loss: 0.4798034980135449\n",
      "Step   295 of   469: \n",
      "Train loss: 0.47815631809881176\n",
      "Test loss: 0.47815631809881176\n",
      "Step   300 of   469: \n",
      "Train loss: 0.4785119479490539\n",
      "Test loss: 0.478602801591663\n",
      "Step   305 of   469: \n",
      "Train loss: 0.47610038215831174\n",
      "Test loss: 0.47677513275106076\n",
      "Step   310 of   469: \n",
      "Train loss: 0.475517964716685\n",
      "Test loss: 0.47593226776284686\n",
      "Step   315 of   469: \n",
      "Train loss: 0.4739085259073872\n",
      "Test loss: 0.474296631701922\n",
      "Step   320 of   469: \n",
      "Train loss: 0.47122089337494416\n",
      "Test loss: 0.4714904500771377\n",
      "Step   325 of   469: \n",
      "Train loss: 0.47025581386129733\n",
      "Test loss: 0.47092799463514556\n",
      "Step   330 of   469: \n",
      "Train loss: 0.46943571001796397\n",
      "Test loss: 0.4699709097207603\n",
      "Step   335 of   469: \n",
      "Train loss: 0.46925336046744204\n",
      "Test loss: 0.4693315130169109\n",
      "Step   340 of   469: \n",
      "Train loss: 0.4684502017700066\n",
      "Test loss: 0.4687381235219665\n",
      "Step   345 of   469: \n",
      "Train loss: 0.46619904091802694\n",
      "Test loss: 0.46709389959351494\n",
      "Step   350 of   469: \n",
      "Train loss: 0.4669332271915371\n",
      "Test loss: 0.46598587475590786\n",
      "Step   355 of   469: \n",
      "Train loss: 0.4662953832391965\n",
      "Test loss: 0.46567575052633126\n",
      "Step   360 of   469: \n",
      "Train loss: 0.46498251112840944\n",
      "Test loss: 0.4652663575390638\n"
     ]
    }
   ],
   "source": [
    "!python3 -u main_mnist.py | tee mnist.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Run CNN on MNIST\n",
      "====================================================================================================\n",
      "Config:\n",
      "dataset_name         (mnist)\n",
      "seed                 (1)\n",
      "verbosity            (WARN)\n",
      "print_step           (100)\n",
      "lift_dimension       (1)\n",
      "num_train            (60000)\n",
      "num_test             (10000)\n",
      "num_channels         (5)\n",
      "filter_size          (3)\n",
      "padding              (same)\n",
      "num_layers           (2)\n",
      "rho                  (0.03)\n",
      "reg                  (0.001)\n",
      "activation           (tanh)\n",
      "delta                (0.5)\n",
      "batch_size           (128)\n",
      "buffer_size          (512)\n",
      "num_epochs           (1)\n",
      "sgd_lr               (0.01)\n",
      "adagrad_lr           (0.01)\n",
      "adam_lr              (0.001)\n",
      "msa_maxiter          (10)\n",
      "msa_perturb_init     (0.0)\n",
      "====================================================================================================\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_0 (Conv2D)            (None, 28, 28, 5)         50        \n",
      "_________________________________________________________________\n",
      "avg_pool_0 (AveragePooling2D (None, 14, 14, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_init (Conv2D)         (None, 14, 14, 5)         230       \n",
      "_________________________________________________________________\n",
      "avg_pool_1 (AveragePooling2D (None, 7, 7, 5)           0         \n",
      "_________________________________________________________________\n",
      "residualconv2d_0 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "residualconv2d_1 (ResidualCo (None, 7, 7, 5)           230       \n",
      "_________________________________________________________________\n",
      "lower (Lower)                (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                500       \n",
      "=================================================================\n",
      "Total params: 1,240\n",
      "Trainable params: 1,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "====================================================================================================\n",
      "Trainer: MSATrainer (MSA_trainer)\n",
      "Settings: {'maxiter': 10, 'perturb_init': 0.0}\n",
      "====================================================================================================\n",
      "Epoch: init\n",
      "Train loss: 2.3245041087522345\n",
      "Test loss: 2.3243458390235903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-951ca62485c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mnist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-836a9731c1d5>\u001b[0m in \u001b[0;36mrun_mnist\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'buffer_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         print_step=config['print_step'])\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# SGD trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, trainset, testset, batch_size, num_epochs, buffer_size, print_step)\u001b[0m\n\u001b[1;32m     89\u001b[0m             self._train_epoch(\n\u001b[1;32m     90\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 batch_size, buffer_size, print_step)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, session, trainset, testset, batch_size, buffer_size, print_step)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprint_step\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step {:5} of {:5}: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebooks/ML/Bauman ml code/Olesya_task/msalib/train.py\u001b[0m in \u001b[0;36m_compute_loss\u001b[0;34m(self, session, dataset, buffer_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             }\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = yaml.safe_load(open(\"config.yml\"))\n",
    "    run_mnist(config['mnist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
